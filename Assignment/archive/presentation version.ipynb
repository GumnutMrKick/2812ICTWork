{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ed0ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#program setup\n",
    "\n",
    "# imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# ransac package\n",
    "from skimage.measure import ransac\n",
    "from skimage.transform import ProjectiveTransform, AffineTransform\n",
    "\n",
    "import time\n",
    "\n",
    "# generates matches between 2 given images using the following method:\n",
    "# complete step 2: using sift to detect local features in an image\n",
    "# complete step 3: knn tree and ratio testing to select good points\n",
    "# complete step 4: use ransac inorder to detect inliers in the two images\n",
    "# it then returns these inlier points\n",
    "def getMatches(base_img, new_img):\n",
    "\n",
    "    # step 2 use SIFT on the images\n",
    "    # initialise sift object\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    # run sift\n",
    "    kp1, des1 = sift.detectAndCompute(base_img, None)\n",
    "    kp2, des2 = sift.detectAndCompute(new_img, None)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #ass\n",
    "    gray = cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY)\n",
    "    kpImg = cv2.drawKeypoints(gray.copy(), kp1, base_img.copy(), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    cv2.imwrite(\"step11.jpg\", kpImg)\n",
    "    gray2 = cv2.cvtColor(new_img, cv2.COLOR_BGR2GRAY)\n",
    "    kpImg2 = cv2.drawKeypoints(gray2.copy(), kp2, new_img.copy(), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    cv2.imwrite(\"step12.jpg\", kpImg2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # set parameters\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks = 50)\n",
    "\n",
    "    # step 3 use KNN \n",
    "    # initialise KNN object\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    \n",
    "    # run KNN tree\n",
    "    matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "    # ratio test to gather good points\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            good.append(m)\n",
    "          \n",
    "    # add these good points too both point holders\n",
    "    base_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1, 2)\n",
    "    new_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1, 2)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #ass\n",
    "    pt2 = [cv2.KeyPoint(point[0], point[1], 1) for point in base_pts]\n",
    "    pt1 = [cv2.KeyPoint(point[0], point[1], 1) for point in new_pts]\n",
    "    #ass\n",
    "    zgray = cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY)\n",
    "    zkpImg = cv2.drawKeypoints(zgray.copy(), pt2, base_img.copy(), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    cv2.imwrite(\"step21.jpg\", zkpImg)\n",
    "    zgray2 = cv2.cvtColor(new_img, cv2.COLOR_BGR2GRAY)\n",
    "    zkpImg2 = cv2.drawKeypoints(zgray2.copy(), pt1, new_img.copy(), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    cv2.imwrite(\"step22.jpg\", zkpImg2)\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # run Ransac to find inliers\n",
    "    model, inliers = ransac((base_pts, new_pts), AffineTransform, min_samples=4, residual_threshold=8, max_trials=10000)\n",
    "    \n",
    "    n_inliers = np.sum(inliers)\n",
    "\n",
    "    inlier_keypoints_base = [cv2.KeyPoint(point[0], point[1], 1) for point in base_pts[inliers]]\n",
    "    inlier_keypoints_new = [cv2.KeyPoint(point[0], point[1], 1) for point in new_pts[inliers]]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #ass\n",
    "    zzgray = cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY)\n",
    "    zzkpImg = cv2.drawKeypoints(zzgray.copy(), inlier_keypoints_base, base_img.copy(), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    cv2.imwrite(\"step31.jpg\", zzkpImg)\n",
    "    zzgray2 = cv2.cvtColor(new_img, cv2.COLOR_BGR2GRAY)\n",
    "    zzkpImg2 = cv2.drawKeypoints(zzgray2.copy(), inlier_keypoints_new, new_img.copy(), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    cv2.imwrite(\"step32.jpg\", zzkpImg2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    d_matches = [cv2.DMatch(idx, idx, 1) for idx in range(n_inliers)]\n",
    "\n",
    "    base_pts = np.float32([ inlier_keypoints_base[m.queryIdx].pt for m in d_matches ]).reshape(-1, 2)\n",
    "    new_pts = np.float32([ inlier_keypoints_new[m.trainIdx].pt for m in d_matches ]).reshape(-1, 2)\n",
    "\n",
    "    # return the inlier points\n",
    "    return base_pts, new_pts\n",
    "\n",
    "# takes in two images and their inlier points, finds the homography required to fix\n",
    "# the destination picture and warps it accordingly while stitching in the source image\n",
    "def stitchImages(base_pts, new_pts, base_img, new_img):\n",
    "    \n",
    "    # find homography\n",
    "    H, masked = cv2.findHomography(new_pts, base_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "    # warp and stitch image\n",
    "    stitched = cv2.warpPerspective(new_img,H,((new_img.shape[1] + base_img.shape[1]), base_img.shape[0])) #wraped image\n",
    "    \n",
    "    \n",
    "    #ass\n",
    "    cv2.imwrite(\"step4.jpg\", stitched)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # copy new image\n",
    "    stitched[0:base_img.shape[0], 0:base_img.shape[1]] = base_img #stitched image\n",
    "\n",
    "    \n",
    "    #ass\n",
    "    cv2.imwrite(\"step5.jpg\", stitched)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # return the new image\n",
    "    return stitched\n",
    "\n",
    "# this function should remove any black borders which maybe present\n",
    "def removeBorder(img):\n",
    "    \n",
    "    # generate threshold\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "  \n",
    "    not_needed,thresh = cv2.threshold(gray,1,255,cv2.THRESH_BINARY)\n",
    "\n",
    "    # find contours\n",
    "    contours,hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # make rectangles\n",
    "    x,y,w,h = cv2.boundingRect(contours[0])\n",
    "\n",
    "    \n",
    "    \n",
    "    #ass\n",
    "    i = img.copy()\n",
    "    cv2.rectangle(i, (x, y), (w, h), (255,0,0), 2)\n",
    "    cv2.imwrite(\"step6.jpg\", i)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # crop the image\n",
    "    crop = img[y:y+h,x:x+w]\n",
    "\n",
    "    \n",
    "    \n",
    "    #ass\n",
    "    cv2.imwrite(\"step7.jpg\", crop)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # return the cropped image\n",
    "    return crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2662ca73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in images\n",
    "imgs = []\n",
    "\n",
    "for x in range(1, 6):\n",
    "    imgs.append(cv2.imread(\"img_\"+str(x)+\".jpg\", cv2.COLOR_RGBA2BGRA))\n",
    "\n",
    "    # use the first image as the base\n",
    "base = imgs.pop(0).copy()\n",
    "\n",
    "# write the original for comparison\n",
    "cv2.imwrite(\"original.jpg\", base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5523b421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches found(281, 2)\n",
      "matches found(272, 2)\n",
      "image rejected as insufficent matches were found\n",
      "matches found(122, 2)\n",
      "the end has been reached\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count = 1\n",
    "\n",
    "# try adding all the images to it\n",
    "while len(imgs) != 0:\n",
    "    \n",
    "    # get new image\n",
    "    new_img = imgs.pop(0).copy()\n",
    "    \n",
    "    # get the matching points        \n",
    "    base_points, new_points = getMatches(base, new_img)\n",
    "    \n",
    "    # match check\n",
    "    if (base_points.shape[0] > 10):\n",
    "\n",
    "        print(\"matches found\" + str(base_points.shape))\n",
    "        \n",
    "        # stitch the image\n",
    "        stitched = stitchImages(base_points, new_points, base, new_img)\n",
    "\n",
    "        # remove the border\n",
    "        base = removeBorder(stitched)\n",
    "\n",
    "        # save progress\n",
    "        cv2.imwrite(\"addition\"+str(count)+\".jpg\", base)\n",
    "\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        #ass\n",
    "        time.sleep(60)\n",
    "               \n",
    "        \n",
    "       \n",
    "\n",
    "    else:\n",
    "        \n",
    "        print(\"image rejected as insufficent matches were found\")\n",
    "        \n",
    "# save output\n",
    "cv2.imwrite(\"output.jpg\", base)\n",
    "        \n",
    "print(\"the end has been reached\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7df39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
